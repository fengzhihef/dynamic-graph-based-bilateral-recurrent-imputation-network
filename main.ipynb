{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca5adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import copy\n",
    "import datetime\n",
    "import pathlib\n",
    "from argparse import Namespace\n",
    "import yaml\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# sys.path.insert(1, os.path.abspath(\"./\"))\n",
    "from lib import fillers, datasets\n",
    "from lib.data.datamodule import SpatioTemporalDataModule\n",
    "from lib.data.imputation_dataset import GraphImputationDataset\n",
    "from lib.nn import models\n",
    "from lib.nn.utils.metric_base import MaskedMetric\n",
    "from lib.nn.utils.metrics import MaskedMAE, MaskedMAPE, MaskedMSE, MaskedMRE\n",
    "from lib.utils import parser_utils, numpy_metrics, ensure_list, prediction_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac4672-19e4-44b2-be17-6f49334bcf95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Z-Score normalization\n",
    "def normalization(data):\n",
    "    temp = np.array(data)\n",
    "    mean = np.nanmean(temp, axis=0)  # Calculate mean ignoring NaNs\n",
    "    std = np.nanstd(temp, axis=0)    # Calculate std ignoring NaNs\n",
    "    temp_masked = np.ma.masked_invalid(temp)  # Mask NaNs\n",
    "    temp = (temp_masked - mean) / (std + 1e-8)  # Prevent division by zero\n",
    "    temp = temp.filled(np.nan)  # Replace mask with NaNs\n",
    "    return temp, mean, std\n",
    "\n",
    "# Z-Score normalization using given mean and std\n",
    "def normalization_with_min_max(data, mean, std):\n",
    "    temp = np.array(data)\n",
    "    temp_masked = np.ma.masked_invalid(temp)  # Mask NaNs\n",
    "    temp = (temp_masked - mean) / (std + 1e-8)  # Prevent division by zero\n",
    "    temp = temp.filled(np.nan)  # Replace mask with NaNs\n",
    "    return temp, mean, std\n",
    "\n",
    "# Reverse Z-Score normalization\n",
    "def denormalization_with_min_max(data, mean, std):\n",
    "    return data * (std + 1e-8) + mean\n",
    "\n",
    "# Calculate metrics and save results\n",
    "def calculate_metrics(true_data, filled_data, miss_mask, x_min, x_max):\n",
    "    mask_indices = np.where(miss_mask == 1)  # Indices of missing values\n",
    "    \n",
    "    # Metrics for original data\n",
    "    rmse = np.sqrt(np.mean((true_data[mask_indices] - filled_data[mask_indices]) ** 2))\n",
    "    mae = np.mean(np.abs(true_data[mask_indices] - filled_data[mask_indices]))\n",
    "    mape = np.mean(np.abs((true_data[mask_indices] - filled_data[mask_indices]) / true_data[mask_indices])) * 100\n",
    "\n",
    "    # Metrics for normalized data\n",
    "    normalized_true_data, _, _ = normalization_with_min_max(true_data, x_min, x_max)\n",
    "    normalized_filled_data, _, _ = normalization_with_min_max(filled_data, x_min, x_max)\n",
    "\n",
    "    normalized_rmse = np.sqrt(np.mean((normalized_true_data[mask_indices] - normalized_filled_data[mask_indices]) ** 2))\n",
    "    normalized_mae = np.mean(np.abs(normalized_true_data[mask_indices] - normalized_filled_data[mask_indices]))\n",
    "    normalized_mape = np.mean(np.abs((normalized_true_data[mask_indices] - normalized_filled_data[mask_indices]) / normalized_true_data[mask_indices])) * 100\n",
    "    return mae, rmse, mape, normalized_mae, normalized_rmse, normalized_mape\n",
    "\n",
    "\n",
    "# Main model function\n",
    "def DGBRIN(X, TEST, save=False):\n",
    "    ########################################\n",
    "    # Transform data and add data column labels\n",
    "    ########################################\n",
    "    split_idx = [len(X), len(X) + len(TEST)]\n",
    "    X = np.concatenate((X, TEST), axis=0)\n",
    "    df = pd.DataFrame(X)\n",
    "    n = df.shape[1]\n",
    "    col_names = list(range(1, n + 1))\n",
    "    df.columns = col_names\n",
    "    # if the data is none\n",
    "    df.index = pd.date_range(start='2024-01-01', periods=len(df), freq='5T')\n",
    "    df.to_hdf('./datasets/Fly/Fly.h5', key='data', format='table')\n",
    "    \n",
    "    args = parse_args()\n",
    "    args.split_idx = split_idx\n",
    "    print(\"Arguments:\", args)\n",
    "    args = copy.deepcopy(args)\n",
    "    torch.set_num_threads(1)\n",
    "    \n",
    "    model_cls, filler_cls = models.DGBRIN, fillers.GraphFiller\n",
    "    dataset = datasets.MissingValuesData()\n",
    "    dataset.numpy()[:] = fill_nan_linear_interpolation_2D(dataset.numpy(), 3)\n",
    "\n",
    "    ########################################\n",
    "    # Create log directory and save configuration\n",
    "    ########################################\n",
    "    exp_name = f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_{args.seed}\"\n",
    "    logdir = os.path.join('logs', args.dataset_name, args.model_name, exp_name)\n",
    "    \n",
    "    pathlib.Path(logdir).mkdir(parents=True)\n",
    "    with open(os.path.join(logdir, 'config.yaml'), 'w') as fp:\n",
    "        yaml.dump(parser_utils.config_dict_from_args(args), fp, indent=4, sort_keys=True)\n",
    "\n",
    "    ########################################\n",
    "    # Data module construction\n",
    "    ########################################\n",
    "    dataset_cls = GraphImputationDataset\n",
    "    torch_dataset = dataset_cls(*dataset.numpy(return_idx=True),\n",
    "                                mask=dataset.training_mask,\n",
    "                                eval_mask=dataset.eval_mask,\n",
    "                                window=args.window,\n",
    "                                stride=args.stride)\n",
    "    \n",
    "    train_idxs = range(args.split_idx[0] - args.window + 1)\n",
    "    val_idxs = train_idxs\n",
    "    test_idxs = range(args.split_idx[0], args.split_idx[1] - args.window + 1)\n",
    "\n",
    "    data_conf = parser_utils.filter_args(args, SpatioTemporalDataModule, return_dict=True)\n",
    "    dm = SpatioTemporalDataModule(torch_dataset, train_idxs=train_idxs, val_idxs=val_idxs, test_idxs=test_idxs, **data_conf)\n",
    "    dm.setup()\n",
    "\n",
    "    ########################################\n",
    "    # Predictor\n",
    "    ########################################\n",
    "    additional_model_hparams = dict(d_in=dm.d_in, n_nodes=dm.n_nodes)\n",
    "    model_kwargs = parser_utils.filter_args(args={**vars(args), **additional_model_hparams},\n",
    "                                            target_cls=model_cls,\n",
    "                                            return_dict=True)\n",
    "    model_kwargs['number_of_samples'] = torch_dataset.data.shape[0] * 2\n",
    "    model_kwargs['time_step'] = TIMESTEPS\n",
    "    model_kwargs['fuse_dim'] = FUSE_DIM\n",
    "\n",
    "    loss_fn = MaskedMetric(metric_fn=getattr(F, args.loss_fn),\n",
    "                           compute_on_step=True,\n",
    "                           metric_kwargs={'reduction': 'none'})\n",
    "   \n",
    "    metrics = {\n",
    "        'mae': MaskedMAE(compute_on_step=False),\n",
    "        'mape': MaskedMAPE(compute_on_step=False),\n",
    "        'mse': MaskedMSE(compute_on_step=False),\n",
    "        'mre': MaskedMRE(compute_on_step=False)\n",
    "    }\n",
    "\n",
    "    scheduler_class = CosineAnnealingLR if args.use_lr_schedule else None\n",
    "    additional_filler_hparams = dict(model_class=model_cls,\n",
    "                                     model_kwargs=model_kwargs,\n",
    "                                     optim_class=torch.optim.Adam,\n",
    "                                     optim_kwargs={'lr': args.lr, 'weight_decay': args.l2_reg},\n",
    "                                     loss_fn=loss_fn,\n",
    "                                     metrics=metrics,\n",
    "                                     scheduler_class=scheduler_class,\n",
    "                                     scheduler_kwargs={'eta_min': 0.0001, 'T_max': args.epochs},\n",
    "                                     alpha=args.alpha,\n",
    "                                     hint_rate=args.hint_rate,\n",
    "                                     g_train_freq=args.g_train_freq,\n",
    "                                     d_train_freq=args.d_train_freq)\n",
    "    filler_kwargs = parser_utils.filter_args(args={**vars(args), **additional_filler_hparams},\n",
    "                                             target_cls=filler_cls,\n",
    "                                             return_dict=True)\n",
    "    filler = filler_cls(**filler_kwargs)\n",
    "\n",
    "    ########################################\n",
    "    # Training\n",
    "    ########################################\n",
    "    early_stop_callback = EarlyStopping(monitor='val_mae', patience=args.patience, mode='min')\n",
    "    checkpoint_callback = ModelCheckpoint(dirpath=logdir, save_top_k=1, monitor='val_mae', mode='min')\n",
    "\n",
    "    logger = TensorBoardLogger(logdir, name=\"model\")\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=args.epochs,\n",
    "                         logger=logger,\n",
    "                         default_root_dir=logdir,\n",
    "                         gradient_clip_val=args.grad_clip_val,\n",
    "                         gradient_clip_algorithm=args.grad_clip_algorithm,\n",
    "                         callbacks=[early_stop_callback, checkpoint_callback])\n",
    "    trainer.fit(filler, datamodule=dm)\n",
    "\n",
    "    ########################################\n",
    "    # Testing\n",
    "    ########################################\n",
    "    filler.load_state_dict(torch.load(checkpoint_callback.best_model_path, lambda storage, loc: storage)['state_dict'])\n",
    "    filler.freeze()\n",
    "    trainer.test(model=filler, datamodule=dm)\n",
    "    filler.eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        filler.cuda()\n",
    "    with torch.no_grad():\n",
    "        y_true, y_hat, mask = filler.predict_loader(dm.test_dataloader(), return_mask=True)\n",
    "    y_hat = y_hat.detach().cpu().numpy().reshape(y_hat.shape[:3])\n",
    "    y_true = y_true.detach().cpu().numpy().reshape(y_hat.shape[:3])\n",
    "    mask = mask.detach().cpu().numpy().reshape(y_hat.shape[:3])\n",
    "\n",
    "    train_mask = dataset.mask[dm.test_slice]\n",
    "    eval_mask_ = dataset.eval_mask[dm.test_slice]\n",
    "    eval_mask = 1 - (train_mask | eval_mask_)\n",
    "    df_true = dataset.df.iloc[dm.test_slice]\n",
    "    metrics = {\n",
    "        'mae': numpy_metrics.masked_mae,\n",
    "        'mse': numpy_metrics.masked_mse,\n",
    "        'mre': numpy_metrics.masked_mre,\n",
    "        'mape': numpy_metrics.masked_mape\n",
    "    }\n",
    "\n",
    "    index = dm.torch_dataset.data_timestamps(dm.testset.indices, flatten=False)['horizon']\n",
    "    aggr_methods = ensure_list(args.aggregate_by)\n",
    "    df_hats = prediction_dataframe(y_hat, index, dataset.df.columns, aggregate_by=aggr_methods)\n",
    "    \n",
    "    df_hats = dict(zip(aggr_methods, df_hats))\n",
    "    for aggr_by, df_hat in df_hats.items():\n",
    "        print(f'- AGGREGATE BY {aggr_by.upper()}')\n",
    "        for metric_name, metric_fn in metrics.items():\n",
    "            error = metric_fn(df_hat.values, df_true.values, eval_mask).item()\n",
    "            print(f' {metric_name}: {error:.4f}')\n",
    "    return df_hat.values, eval_mask\n",
    "\n",
    "# Linear interpolation for 2D data to pre-fill NaNs\n",
    "def fill_nan_linear_interpolation_2D(data, prev_count):\n",
    "    for col in range(data.shape[1]):\n",
    "        arr = data[:, col]\n",
    "        n = len(arr)\n",
    "        start_value = None\n",
    "        end_value = None\n",
    "        for i in range(n):\n",
    "            if np.isnan(arr[i]):\n",
    "                prev_values = []\n",
    "                prev_indices = []\n",
    "                for j in range(i-1, -1, -1):\n",
    "                    if len(prev_values) >= prev_count:\n",
    "                        break\n",
    "                    if not np.isnan(arr[j]):\n",
    "                        prev_values.append(arr[j])\n",
    "                        prev_indices.append(j)\n",
    "                next_value = None\n",
    "                next_index = None\n",
    "                for j in range(i+1, n):\n",
    "                    if not np.isnan(arr[j]):\n",
    "                        next_value = arr[j]\n",
    "                        next_index = j\n",
    "                        break\n",
    "                if i == 0 and len(prev_values) == 0 and next_value is not None:\n",
    "                    start_value = next_value\n",
    "                if i == n - 1 and next_value is None and len(prev_values) > 0:\n",
    "                    end_value = prev_values[-1]\n",
    "                if len(prev_values) == 0 or next_value is None:\n",
    "                    continue\n",
    "                x = prev_indices + [next_index]\n",
    "                y = prev_values + [next_value]\n",
    "                f = interp1d(x, y, kind='linear')\n",
    "                arr[i] = f(i)\n",
    "        if start_value is not None and np.isnan(arr[0]):\n",
    "            for index in range(len(arr)):\n",
    "                if np.isnan(arr[index]):\n",
    "                    arr[index] = start_value\n",
    "                else:\n",
    "                    break\n",
    "        if end_value is not None and np.isnan(arr[-1]):\n",
    "            for index in range(len(arr) - 1, -1, -1):\n",
    "                if np.isnan(arr[index]):\n",
    "                    arr[index] = end_value\n",
    "                else:\n",
    "                    break\n",
    "        data[:, col] = arr\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b4ce3f-50c4-4481-861b-65083c9dfe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and file paths\n",
    "MODEL_NAME = 'DGBRIN'\n",
    "FUSE_DIM = 30\n",
    "TIMESTEPS = 30\n",
    "TRAIN_DATA_PATH = f'timeseries_datasets/example_train.csv'\n",
    "TEST_DATA_PATH = f'timeseries_datasets/example_test.csv'\n",
    "REAL_DATA_PATH = f'timeseries_datasets/example_real.csv'\n",
    "\n",
    "# Parse arguments for the model\n",
    "def parse_args():\n",
    "    args_dict = {\n",
    "        \"seed\": 1,\n",
    "        \"model_name\": 'DGBRIN',\n",
    "        \"dataset_name\": 'fly',\n",
    "        \"in_sample\": False,\n",
    "        \"val_len\": 0.1,\n",
    "        \"test_len\": 0,\n",
    "        \"aggregate_by\": 'mean',\n",
    "        \"lr\": 0.001,\n",
    "        \"epochs\": 200,\n",
    "        \"patience\": 20,\n",
    "        \"l2_reg\": 0.,\n",
    "        \"scaled_target\": True,\n",
    "        \"grad_clip_val\": 5.,\n",
    "        \"grad_clip_algorithm\": 'norm',\n",
    "        \"loss_fn\": 'l1_loss',\n",
    "        \"use_lr_schedule\": True,\n",
    "        \"consistency_loss\": False,\n",
    "        \"whiten_prob\": 0.05,\n",
    "        \"pred_loss_weight\": 1.0,\n",
    "        \"warm_up\": 0,\n",
    "        \"adj_threshold\": 0.1,\n",
    "        \"alpha\": 20.,\n",
    "        \"hint_rate\": 0.7,\n",
    "        \"g_train_freq\": 1,\n",
    "        \"d_train_freq\": 5,\n",
    "        \"batch_size\": 32,\n",
    "        \"window\": TIMESTEPS,\n",
    "        \"horizon\": 24,\n",
    "        \"delay\": 0,\n",
    "        \"stride\": 1,\n",
    "        \"scaling_axis\": \"channels\",\n",
    "        \"scaling_type\": \"std\",\n",
    "        \"scale\": True,\n",
    "        \"workers\": 0,\n",
    "        \"samples_per_epoch\": None,\n",
    "    }\n",
    "    args = Namespace(**args_dict)\n",
    "    model_cls = models.DGBRIN\n",
    "    args = model_cls.add_model_specific_args(args)\n",
    "    return args\n",
    "\n",
    "\n",
    "# Load data\n",
    "X = np.array(pd.read_csv(TRAIN_DATA_PATH))\n",
    "TEST = np.array(pd.read_csv(TEST_DATA_PATH))\n",
    "R = np.array(pd.read_csv(REAL_DATA_PATH))\n",
    "\n",
    "# Normalize data\n",
    "x_zz_norm, x_min, x_max = normalization(X.copy())  # Use normalized data with missing values\n",
    "test_zz_norm, _, _ = normalization_with_min_max(TEST, x_min, x_max)\n",
    "\n",
    "# Model inference\n",
    "time_steps, num_nodes = TEST.shape\n",
    "y_zz_norm, MASK = DGBRIN(x_zz_norm, test_zz_norm, save=True)  # Model inference with normalized data\n",
    "Y = denormalization_with_min_max(y_zz_norm, x_min, x_max)  # Denormalize results\n",
    "\n",
    "# Analyze and save results\n",
    "mae, rmse, mape, normalized_mae, normalized_rmse, normalized_mape = calculate_metrics(\n",
    "    R, Y, MASK, x_min, x_max)\n",
    "\n",
    "# Print results\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAPE:\", mape)\n",
    "print(\"Normalized MAE:\", normalized_mae)\n",
    "print(\"Normalized RMSE:\", normalized_rmse)\n",
    "print(\"Normalized MAPE:\", normalized_mape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
